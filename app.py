# app.py

import os
import re
import json
import openai
import tiktoken
import numpy as np
import streamlit as st
from PyPDF2 import PdfReader
from typing import List, Dict
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import silhouette_score
from collections import defaultdict, Counter
from zipfile import ZipFile

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. ì„¤ì •
OPENAI_API_KEY = st.secrets["OPENAI_API_KEY"]
EMBED_MODEL = "text-embedding-3-small"
GPT_ANALYZER_MODEL = "gpt-5-nano"
GPT_STRUCTURER_MODEL = "gpt-3.5-turbo"
CHUNK_TOKEN_SIZE = 500
MIN_CLUSTER_SIZE = 2
RECOMMEND_TOP_N = 3
OUTPUT_ZIP_PATH = "outputs/summaries.zip"

openai.api_key = OPENAI_API_KEY
for path in ["data", "outputs"]:
    os.makedirs(path, exist_ok=True)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. ìœ í‹¸
def count_tokens(text: str) -> int:
    enc = tiktoken.get_encoding("cl100k_base")
    return len(enc.encode(text))

def load_file(file) -> str:
    ext = os.path.splitext(file.name)[-1].lower()
    if ext == ".pdf":
        return "\n".join([page.extract_text() or "" for page in PdfReader(file).pages])
    return file.read().decode("utf-8")

def split_chunks(text: str, max_tokens: int = CHUNK_TOKEN_SIZE) -> List[str]:
    sentences = re.split(r"(?<=[.!?]) +", text)
    chunks, current = [], ""
    for sent in sentences:
        tentative = f"{current} {sent}".strip() if current else sent
        if count_tokens(tentative) <= max_tokens:
            current = tentative
        else:
            if current: chunks.append(current)
            current = sent
    if current: chunks.append(current)
    return chunks

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. ì„ë² ë”©
def get_embedding(text: str) -> list[float]:
    return openai.Embedding.create(model=EMBED_MODEL, input=text)['data'][0]['embedding']

def process_and_store_embeddings(chunks: List[str], doc_id: str):
    vectors = [get_embedding(c) for c in chunks]
    avg_vector = [sum(x)/len(x) for x in zip(*vectors)]
    return avg_vector

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. í´ëŸ¬ìŠ¤í„°ë§
def determine_best_k(vectors, k_range=(2, 8)) -> int:
    best_k, best_score = k_range[0], -1
    for k in range(*k_range):
        km = KMeans(n_clusters=k, random_state=42, n_init="auto")
        labels = km.fit_predict(vectors)
        score = silhouette_score(vectors, labels)
        if score > best_score:
            best_k, best_score = k, score
    return best_k

def cluster_embeddings(vectors: List[List[float]], doc_ids: List[str], auto_k=True, fixed_k=4) -> Dict[str, int]:
    X = np.array(vectors)
    k = determine_best_k(X) if auto_k else fixed_k
    km = KMeans(n_clusters=k, random_state=42, n_init="auto")
    labels = km.fit_predict(X)
    return {doc_id: int(cid) for doc_id, cid in zip(doc_ids, labels)}

def merge_small_clusters(cluster_map: Dict[str, int], min_size=MIN_CLUSTER_SIZE) -> Dict[str, int]:
    counter = Counter(cluster_map.values())
    small = [cid for cid, cnt in counter.items() if cnt < min_size]
    major = counter.most_common(1)[0][0]
    return {doc_id: (major if cid in small else cid) for doc_id, cid in cluster_map.items()}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5. GPT í•´ì„
def summarize_cluster(document_texts: list[str]) -> dict:
    content = "\n\n".join(document_texts)
    sys = """ë‹¹ì‹ ì€ ì—¬ëŸ¬ ë¬¸ì„œë¥¼ ë¶„ì„í•˜ì—¬ ê³µí†µëœ ì˜ë¯¸ë¥¼ ì •ë¦¬í•˜ëŠ” ì •ë³´ ë¶„ì„ê°€ì…ë‹ˆë‹¤.
ê·œì¹™:
- ì‚¬ê³  ê³¼ì •ì´ë‚˜ ë¶„ì„ ì´ìœ ë¥¼ ì ˆëŒ€ ì„¤ëª…í•˜ì§€ ë§ˆì„¸ìš”.
- ê°œë³„ ë¬¸ì„œë¥¼ ì§ì ‘ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”.
- ì—¬ëŸ¬ ë¬¸ì„œì— ê³µí†µì ìœ¼ë¡œ ë‚˜íƒ€ë‚˜ëŠ” í•µì‹¬ ì˜ë¯¸ë§Œ ì¶”ì¶œí•˜ì„¸ìš”.
- ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.
- ì¶œë ¥ì€ ë°˜ë“œì‹œ JSON í˜•ì‹ë§Œ ì‚¬ìš©í•˜ì„¸ìš”."""
    user = f"""
ì•„ë˜ëŠ” ë™ì¼í•œ ì˜ë¯¸ì  í´ëŸ¬ìŠ¤í„°ì— ì†í•œ ì—¬ëŸ¬ ë¬¸ì„œì˜ ë‚´ìš©ì…ë‹ˆë‹¤.

ë¬¸ì„œ ë‚´ìš©:
{content}

ì‘ì—… ì§€ì‹œ:
1. ì´ ë¬¸ì„œ ë¬¶ìŒì„ ëŒ€í‘œí•˜ëŠ” í´ëŸ¬ìŠ¤í„° ì£¼ì œë¥¼ í•˜ë‚˜ ìƒì„±í•˜ì„¸ìš”. (ìµœëŒ€ 12ë‹¨ì–´)
2. í´ëŸ¬ìŠ¤í„° ì „ì²´ë¥¼ ìš”ì•½í•˜ëŠ” ë¬¸ì¥ì„ 3~5ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
3. í´ëŸ¬ìŠ¤í„°ë¥¼ ê°€ì¥ ì˜ ì„¤ëª…í•˜ëŠ” í•µì‹¬ í‚¤ì›Œë“œ 5~8ê°œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.

ì¶œë ¥ í˜•ì‹ (JSONë§Œ):
{{
  "cluster_topic": "",
  "cluster_summary": "",
  "keywords": []
}}
"""
    res = openai.ChatCompletion.create(
        model=GPT_ANALYZER_MODEL,
        messages=[{"role": "system", "content": sys}, {"role": "user", "content": user}],
        temperature=0.4
    )
    return json.loads(res.choices[0].message.content)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 6. ì¶”ì²œ
def recommend_by_cosine(new_vec, existing_vecs, top_n=RECOMMEND_TOP_N):
    sims = cosine_similarity([new_vec], existing_vecs)[0]
    top_idxs = np.argsort(sims)[::-1][:top_n]
    return top_idxs, sims[top_idxs]

def explain_document_similarity(target_doc: str, related_docs: list[tuple[str, str]]) -> dict:
    rel_txt = "\n".join([f"- {doc_id}: {text}" for doc_id, text in related_docs])
    user_prompt = f"""ê¸°ì¤€ ë¬¸ì„œ:
{target_doc}

ì—°ê´€ ë¬¸ì„œ ëª©ë¡:
{rel_txt}

ì‘ì—… ì§€ì‹œ:
ê° ì—°ê´€ ë¬¸ì„œê°€ ê¸°ì¤€ ë¬¸ì„œì™€ ì™œ í•¨ê»˜ ì½ìœ¼ë©´ ì¢‹ì€ì§€ í•œ ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.

ì¶œë ¥ í˜•ì‹ (JSONë§Œ):
{{
  "recommendations": [
    {{
      "document_id": "",
      "reason": ""
    }}
  ]
}}"""
    sys = """ë‹¹ì‹ ì€ ë¬¸ì„œ ê°„ì˜ ê³µí†µ ì£¼ì œë¥¼ ê°„ë‹¨íˆ ì„¤ëª…í•˜ëŠ” ë¶„ì„ê°€ì…ë‹ˆë‹¤.
ê·œì¹™:
- ë¬¸ì„œ ë‚´ìš©ì„ ìš”ì•½í•˜ê±°ë‚˜ ì¬ì‘ì„±í•˜ì§€ ë§ˆì„¸ìš”.
- ê³µí†µëœ ì£¼ì œ ë˜ëŠ” ê´€ì ë§Œ í•œ ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.
- ì¶œë ¥ì€ ë°˜ë“œì‹œ JSON í˜•ì‹ë§Œ ì‚¬ìš©í•˜ì„¸ìš”."""
    res = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "system", "content": sys}, {"role": "user", "content": user_prompt}],
        temperature=0.3
    )
    return json.loads(res.choices[0].message.content)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 7. Streamlit UI
st.set_page_config(page_title="ğŸ“„ ë¬¸ì„œ ë¶„ì„", layout="wide")
st.title("ğŸ“„ ë¬¸ì„œ ì˜ë¯¸ ë¶„ì„ ë° ì¶”ì²œ í”Œë«í¼")

if "doc_texts" not in st.session_state:
    st.session_state.doc_texts = {}
    st.session_state.doc_vectors = {}

uploaded_files = st.file_uploader("ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš” (.pdf, .txt, .md)", type=["pdf", "txt", "md"], accept_multiple_files=True)

if uploaded_files:
    with st.spinner("ë¬¸ì„œ ì—…ë¡œë“œ ë° ì„ë² ë”© ì²˜ë¦¬ ì¤‘..."):
        progress = st.progress(0)
        for i, file in enumerate(uploaded_files):
            doc_id = file.name
            st.markdown(f"ğŸ“„ `{doc_id}` ì²˜ë¦¬ ì¤‘...")
            text = load_file(file)
            chunks = split_chunks(text)
            avg_vec = process_and_store_embeddings(chunks, doc_id)
            st.session_state.doc_texts[doc_id] = text
            st.session_state.doc_vectors[doc_id] = avg_vec
            progress.progress((i + 1) / len(uploaded_files))
        st.success("âœ… ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ!")

if st.button("ğŸš€ ì˜ë¯¸ ê¸°ë°˜ ë¶„ì„ ì‹¤í–‰"):
    with st.spinner("í´ëŸ¬ìŠ¤í„°ë§ ë° GPT ìš”ì•½ ì¤‘..."):
        doc_ids = list(st.session_state.doc_vectors.keys())
        vectors = list(st.session_state.doc_vectors.values())

        cluster_map = cluster_embeddings(vectors, doc_ids)
        cluster_map = merge_small_clusters(cluster_map)

        clusters = defaultdict(list)
        for doc_id, cid in cluster_map.items():
            clusters[cid].append(doc_id)

        summaries = {}
        for cid, doc_list in clusters.items():
            texts = [st.session_state.doc_texts[d] for d in doc_list]
            summary = summarize_cluster(texts)
            summaries[cid] = summary
            with st.expander(f"ğŸ“ í´ëŸ¬ìŠ¤í„° {cid}"):
                st.write(f"ğŸ“Œ ì£¼ì œ: **{summary['cluster_topic']}**")
                st.info(summary['cluster_summary'])
                st.write("ğŸ”‘ í‚¤ì›Œë“œ: " + ", ".join([f"`{kw}`" for kw in summary["keywords"]]))
                st.write("ğŸ“„ ë¬¸ì„œ:")
                for doc in doc_list:
                    st.markdown(f"- {doc}")

        st.subheader("ğŸ“š ìœ ì‚¬ ë¬¸ì„œ ì¶”ì²œ")
        if len(doc_ids) > 1:
            target = doc_ids[0]
            target_vec = st.session_state.doc_vectors[target]
            others = [v for i, v in enumerate(vectors) if doc_ids[i] != target]
            other_ids = [d for d in doc_ids if d != target]
            top_idxs, _ = recommend_by_cosine(target_vec, others)
            top_docs = [other_ids[i] for i in top_idxs]
            related = [(d, st.session_state.doc_texts[d]) for d in top_docs]
            reasons = explain_document_similarity(st.session_state.doc_texts[target], related)
            for r in reasons["recommendations"]:
                st.markdown(f"ğŸ”— **{r['document_id']}**: {r['reason']}")

        with ZipFile(OUTPUT_ZIP_PATH, "w") as zipf:
            for cid, data in summaries.items():
                md = f"# í´ëŸ¬ìŠ¤í„° {cid}\n\n"
                md += f"**ì£¼ì œ:** {data['cluster_topic']}\n\n"
                md += f"**ìš”ì•½:**\n{data['cluster_summary']}\n\n"
                md += f"**í‚¤ì›Œë“œ:** {', '.join(data['keywords'])}\n"
                zipf.writestr(f"cluster_{cid}.md", md)

        with open(OUTPUT_ZIP_PATH, "rb") as f:
            st.download_button("ğŸ“¦ ìš”ì•½ ê²°ê³¼ ZIP ë‹¤ìš´ë¡œë“œ", f, file_name="summaries.zip")
